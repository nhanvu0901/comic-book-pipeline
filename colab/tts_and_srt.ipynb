{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ™ï¸ Comic Video Pipeline â€” Stage 3 & 4: TTS + SRT\n",
    "\n",
    "This notebook generates narration audio using **Qwen3-TTS** (voice cloning) and SRT subtitles using **Whisper**.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run Stage 1 (script) and Stage 2 (images) on your Mac first\n",
    "- Script and images should be in Google Drive: `comic-pipeline/<project_name>/`\n",
    "- Prepare a **3-10 second WAV voice sample** for voice cloning\n",
    "\n",
    "**Flow:**\n",
    "1. Mount Google Drive\n",
    "2. Select project & upload voice sample\n",
    "3. Generate per-scene audio with Qwen3-TTS\n",
    "4. (Optional) Generate word-level SRT with Whisper\n",
    "5. Files auto-sync back to your Mac via Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup: Install Dependencies & Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_BASE = '/content/drive/MyDrive/comic-pipeline'\n",
    "\n",
    "import os\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "print('âœ… Google Drive mounted')\n",
    "print(f'ğŸ“ Base path: {DRIVE_BASE}')\n",
    "\n",
    "# List available projects\n",
    "projects = [d for d in os.listdir(DRIVE_BASE) \n",
    "            if os.path.isdir(os.path.join(DRIVE_BASE, d))\n",
    "            and os.path.exists(os.path.join(DRIVE_BASE, d, 'script.json'))]\n",
    "print(f'\\nğŸ“‹ Available projects: {projects}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SET YOUR PROJECT NAME HERE =====\n",
    "PROJECT_NAME = 'death_of_gwen_stacy'  # <-- CHANGE THIS\n",
    "# ======================================\n",
    "\n",
    "PROJECT_DIR = os.path.join(DRIVE_BASE, PROJECT_NAME)\n",
    "AUDIO_DIR = os.path.join(PROJECT_DIR, 'audio')\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "\n",
    "# Load script\n",
    "import json\n",
    "with open(os.path.join(PROJECT_DIR, 'script.json')) as f:\n",
    "    script = json.load(f)\n",
    "\n",
    "print(f'ğŸ“– Loaded: {script.get(\"title\", PROJECT_NAME)}')\n",
    "print(f'ğŸ¬ Scenes: {len(script[\"scenes\"])}')\n",
    "for s in script['scenes']:\n",
    "    print(f'   Scene {s[\"scene_id\"]}: {s[\"narration\"][:60]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Install Qwen3-TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Qwen3-TTS dependencies\n",
    "!pip install transformers accelerate soundfile torch torchaudio\n",
    "!pip install cosyvoice-ttsfrd\n",
    "print('âœ… Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(f'ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\n",
    "print(f'ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB' if torch.cuda.is_available() else '')\n",
    "\n",
    "# Load Qwen3-TTS model\n",
    "print('\\nâ³ Loading Qwen3-TTS model (this may take 2-5 minutes)...')\n",
    "\n",
    "MODEL_ID = 'Qwen/Qwen3-TTS-12Hz-1.7B-Base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print('âœ… Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Upload Voice Sample for Cloning\n",
    "\n",
    "Upload a **3-10 second WAV file** of the voice you want to clone.\n",
    "For a dramatic comic narrator, use a sample of a deep, authoritative voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import torchaudio\n",
    "\n",
    "# Option 1: Upload from your computer\n",
    "print('ğŸ“¤ Upload a voice sample WAV file (3-10 seconds):')\n",
    "uploaded = files.upload()\n",
    "\n",
    "VOICE_SAMPLE_PATH = None\n",
    "for filename in uploaded:\n",
    "    VOICE_SAMPLE_PATH = f'/content/{filename}'\n",
    "    with open(VOICE_SAMPLE_PATH, 'wb') as f:\n",
    "        f.write(uploaded[filename])\n",
    "    break\n",
    "\n",
    "# Option 2: Use a file already in Google Drive\n",
    "# VOICE_SAMPLE_PATH = os.path.join(PROJECT_DIR, 'voice_sample.wav')\n",
    "\n",
    "if VOICE_SAMPLE_PATH and os.path.exists(VOICE_SAMPLE_PATH):\n",
    "    waveform, sr = torchaudio.load(VOICE_SAMPLE_PATH)\n",
    "    duration = waveform.shape[1] / sr\n",
    "    print(f'âœ… Voice sample loaded: {duration:.1f}s, {sr}Hz')\n",
    "else:\n",
    "    print('âš ï¸  No voice sample. Will use default voice.')\n",
    "    VOICE_SAMPLE_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Generate Narration Audio (Per-Scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech(text, voice_sample_path=None, output_path='output.wav'):\n",
    "    \"\"\"\n",
    "    Generate speech from text using Qwen3-TTS.\n",
    "    If voice_sample_path is provided, clone that voice.\n",
    "    \"\"\"\n",
    "    # Build the prompt for Qwen3-TTS\n",
    "    if voice_sample_path:\n",
    "        # Voice cloning mode\n",
    "        waveform, sr = torchaudio.load(voice_sample_path)\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Prepare prompt with voice reference\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': [\n",
    "                {'type': 'audio', 'audio': voice_sample_path},\n",
    "                {'type': 'text', 'text': text}\n",
    "            ]}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': text}\n",
    "        ]\n",
    "    \n",
    "    # Generate with the model\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=4096,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Decode audio tokens to waveform\n",
    "    audio_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    audio_array = tokenizer.decode_audio(audio_tokens)\n",
    "    \n",
    "    # Save\n",
    "    sf.write(output_path, audio_array, 24000)\n",
    "    return output_path\n",
    "\n",
    "print('âœ… TTS function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GENERATE AUDIO FOR ALL SCENES =====\n",
    "print('ğŸ™ï¸  Generating narration for all scenes...\\n')\n",
    "\n",
    "generated_files = []\n",
    "\n",
    "for scene in script['scenes']:\n",
    "    sid = scene['scene_id']\n",
    "    narration = scene['narration']\n",
    "    output_path = os.path.join(AUDIO_DIR, f'scene_{sid:02d}.wav')\n",
    "    \n",
    "    print(f'  Scene {sid}: \"{narration[:50]}...\"')\n",
    "    \n",
    "    try:\n",
    "        generate_speech(\n",
    "            text=narration,\n",
    "            voice_sample_path=VOICE_SAMPLE_PATH,\n",
    "            output_path=output_path\n",
    "        )\n",
    "        \n",
    "        # Verify\n",
    "        waveform, sr = torchaudio.load(output_path)\n",
    "        duration = waveform.shape[1] / sr\n",
    "        print(f'    âœ… Generated: {duration:.1f}s')\n",
    "        generated_files.append(output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'    âŒ Error: {e}')\n",
    "        print(f'    Trying without voice cloning...')\n",
    "        try:\n",
    "            generate_speech(\n",
    "                text=narration,\n",
    "                voice_sample_path=None,\n",
    "                output_path=output_path\n",
    "            )\n",
    "            generated_files.append(output_path)\n",
    "            print(f'    âœ… Generated (default voice)')\n",
    "        except Exception as e2:\n",
    "            print(f'    âŒ Failed completely: {e2}')\n",
    "\n",
    "print(f'\\nâœ… Generated {len(generated_files)}/{len(script[\"scenes\"])} audio files')\n",
    "print(f'ğŸ“ Saved to: {AUDIO_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4bï¸âƒ£ (Alternative) If Qwen3-TTS doesn't work, use Chatterbox\n",
    "\n",
    "Uncomment and run the cells below as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FALLBACK: CHATTERBOX TTS =====\n",
    "# Uncomment all lines below if Qwen3-TTS fails\n",
    "\n",
    "# !pip install chatterbox-tts\n",
    "\n",
    "# import chatterbox\n",
    "# from chatterbox.tts import ChatterboxTTS\n",
    "# \n",
    "# cb_model = ChatterboxTTS.from_pretrained(device='cuda')\n",
    "# print('âœ… Chatterbox loaded')\n",
    "# \n",
    "# for scene in script['scenes']:\n",
    "#     sid = scene['scene_id']\n",
    "#     output_path = os.path.join(AUDIO_DIR, f'scene_{sid:02d}.wav')\n",
    "#     \n",
    "#     wav = cb_model.generate(\n",
    "#         scene['narration'],\n",
    "#         audio_prompt_path=VOICE_SAMPLE_PATH,\n",
    "#         exaggeration=0.6,     # dramatic style\n",
    "#         cfg=0.5,\n",
    "#         temperature=0.8,\n",
    "#     )\n",
    "#     sf.write(output_path, wav.squeeze().cpu().numpy(), 24000)\n",
    "#     print(f'  âœ… Scene {sid} done')\n",
    "# \n",
    "# print('âœ… All scenes generated with Chatterbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Concatenate All Audio into One Narration Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate per-scene audio into one narration file\n",
    "import wave\n",
    "import glob\n",
    "\n",
    "scene_files = sorted(glob.glob(os.path.join(AUDIO_DIR, 'scene_*.wav')))\n",
    "print(f'ğŸ“ Found {len(scene_files)} scene audio files')\n",
    "\n",
    "if scene_files:\n",
    "    # Read all and concatenate\n",
    "    all_audio = []\n",
    "    sample_rate = None\n",
    "    \n",
    "    for f in scene_files:\n",
    "        data, sr = sf.read(f)\n",
    "        if sample_rate is None:\n",
    "            sample_rate = sr\n",
    "        elif sr != sample_rate:\n",
    "            # Resample if needed\n",
    "            import librosa\n",
    "            data = librosa.resample(data, orig_sr=sr, target_sr=sample_rate)\n",
    "        all_audio.append(data)\n",
    "        print(f'  {os.path.basename(f)}: {len(data)/sample_rate:.1f}s')\n",
    "    \n",
    "    combined = np.concatenate(all_audio)\n",
    "    narration_path = os.path.join(AUDIO_DIR, 'narration.wav')\n",
    "    sf.write(narration_path, combined, sample_rate)\n",
    "    \n",
    "    total_duration = len(combined) / sample_rate\n",
    "    print(f'\\nâœ… Combined narration: {total_duration:.1f}s')\n",
    "    print(f'ğŸ’¾ Saved to: {narration_path}')\n",
    "else:\n",
    "    print('âŒ No audio files found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ (Optional) Word-Level SRT with Whisper\n",
    "\n",
    "This gives you precise word-by-word subtitles instead of scene-level.\n",
    "Skip this if scene-level SRT (generated by Stage 5 on your Mac) is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Whisper\n",
    "!pip install openai-whisper\n",
    "\n",
    "import whisper\n",
    "\n",
    "print('â³ Loading Whisper model...')\n",
    "whisper_model = whisper.load_model('base')  # Options: tiny, base, small, medium, large\n",
    "print('âœ… Whisper loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe narration to get word-level timestamps\n",
    "narration_path = os.path.join(AUDIO_DIR, 'narration.wav')\n",
    "\n",
    "if os.path.exists(narration_path):\n",
    "    print('ğŸ” Transcribing with Whisper...')\n",
    "    result = whisper_model.transcribe(\n",
    "        narration_path,\n",
    "        word_timestamps=True,\n",
    "        language='en'\n",
    "    )\n",
    "    \n",
    "    # Save Whisper JSON\n",
    "    whisper_json_path = os.path.join(AUDIO_DIR, 'whisper_transcript.json')\n",
    "    with open(whisper_json_path, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    print(f'ğŸ’¾ Whisper JSON saved: {whisper_json_path}')\n",
    "    \n",
    "    # Generate SRT from Whisper output\n",
    "    def format_time(seconds):\n",
    "        h = int(seconds // 3600)\n",
    "        m = int((seconds % 3600) // 60)\n",
    "        s = int(seconds % 60)\n",
    "        ms = int((seconds % 1) * 1000)\n",
    "        return f'{h:02d}:{m:02d}:{s:02d},{ms:03d}'\n",
    "    \n",
    "    srt_entries = []\n",
    "    for i, seg in enumerate(result['segments'], 1):\n",
    "        start = format_time(seg['start'])\n",
    "        end = format_time(seg['end'])\n",
    "        text = seg['text'].strip()\n",
    "        srt_entries.append(f'{i}\\n{start} --> {end}\\n{text}\\n')\n",
    "    \n",
    "    srt_content = '\\n'.join(srt_entries)\n",
    "    srt_path = os.path.join(AUDIO_DIR, 'whisper_subtitles.srt')\n",
    "    with open(srt_path, 'w') as f:\n",
    "        f.write(srt_content)\n",
    "    \n",
    "    print(f'ğŸ“ Whisper SRT saved: {srt_path}')\n",
    "    print(f'   Segments: {len(result[\"segments\"])}')\n",
    "    print(f'\\n   Preview:')\n",
    "    for seg in result['segments'][:3]:\n",
    "        print(f'   [{seg[\"start\"]:.1f}s - {seg[\"end\"]:.1f}s] {seg[\"text\"].strip()}')\n",
    "else:\n",
    "    print('âŒ Narration file not found. Run Step 5 first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Done!\n",
    "\n",
    "Your files are now in Google Drive and will sync to your Mac:\n",
    "\n",
    "```\n",
    "comic-pipeline/<project>/\n",
    "â”œâ”€â”€ audio/\n",
    "â”‚   â”œâ”€â”€ scene_01.wav          â† per-scene audio\n",
    "â”‚   â”œâ”€â”€ scene_02.wav\n",
    "â”‚   â”œâ”€â”€ ...\n",
    "â”‚   â”œâ”€â”€ narration.wav         â† combined narration\n",
    "â”‚   â”œâ”€â”€ whisper_transcript.json  â† (optional) Whisper output\n",
    "â”‚   â””â”€â”€ whisper_subtitles.srt    â† (optional) word-level SRT\n",
    "```\n",
    "\n",
    "**Next step on your Mac:**\n",
    "```bash\n",
    "python -m stages.stage5_video_assembler --project \"<project_name>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('=' * 60)\n",
    "print('ğŸ“Š SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Project: {PROJECT_NAME}')\n",
    "print(f'Title: {script.get(\"title\", \"N/A\")}')\n",
    "print(f'\\nGenerated files in: {AUDIO_DIR}')\n",
    "\n",
    "for f in sorted(os.listdir(AUDIO_DIR)):\n",
    "    fpath = os.path.join(AUDIO_DIR, f)\n",
    "    size_kb = os.path.getsize(fpath) / 1024\n",
    "    print(f'  ğŸ“„ {f} ({size_kb:.0f} KB)')\n",
    "\n",
    "print(f'\\nğŸ‘‰ Go back to your Mac and run:')\n",
    "print(f'   python -m stages.stage5_video_assembler --project \"{PROJECT_NAME}\"')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
